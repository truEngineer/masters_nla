{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Семинар 4.\n",
    "## Линейная алгебра и нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Нейросеть как суперпозиция функций\n",
    "\n",
    "- Линейных и нелинейных\n",
    "- Можно представить в виде графа вычислений\n",
    "- С помощью этого графа можно [автоматически вычислять градиенты](https://en.wikipedia.org/wiki/Automatic_differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Что такое градиент и как его вычислить?\n",
    "\n",
    "- Если функция $f$ зависит от вектора или матрицы и вычисляет скаляр, то её градиент по аргументу - это вектор или матрица, элементы которых – производные выхода по соответствующему элементу аргумента\n",
    "- Пример: $f(x) = x^{\\top}Ax + b^{\\top}x$. Чему равен градиент?\n",
    "- Поэлементный пример: $f(x) = x^2$ поэлементно. Чему равен градиент (матрица Якоби)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.7053, grad_fn=<SubBackward0>)\n",
      "-3.70530104637146\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n = 5\n",
    "A = torch.randn((n, n), requires_grad=True)\n",
    "b = torch.randn((n,))\n",
    "x = torch.randn((n,), requires_grad=True)\n",
    "\n",
    "f = 0.5 * x @ A @ x - b @ x\n",
    "g = f * f\n",
    "g.backward()\n",
    "print(f)\n",
    "print(f.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0102, -0.7805,  1.1065, -0.3718, -4.2938])\n",
      "tensor([-0.0755,  5.7839, -8.1999,  2.7555, 31.8199])\n"
     ]
    }
   ],
   "source": [
    "manual_grad_x = 0.5 * (A + A.t()) @ x - b\n",
    "\n",
    "print(manual_grad_x.data)\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.6064e-03, -1.7660e-03,  1.4851e-01, -5.8479e-02, -5.7553e-02],\n",
      "        [-1.7660e-03,  3.2464e-04, -2.7301e-02,  1.0750e-02,  1.0580e-02],\n",
      "        [ 1.4851e-01, -2.7301e-02,  2.2959e+00, -9.0405e-01, -8.8974e-01],\n",
      "        [-5.8479e-02,  1.0750e-02, -9.0405e-01,  3.5598e-01,  3.5035e-01],\n",
      "        [-5.7553e-02,  1.0580e-02, -8.8974e-01,  3.5035e-01,  3.4481e-01]])\n",
      "tensor([[-7.1189e-02,  1.3087e-02, -1.1006e+00,  4.3336e-01,  4.2650e-01],\n",
      "        [ 1.3087e-02, -2.4058e-03,  2.0232e-01, -7.9665e-02, -7.8405e-02],\n",
      "        [-1.1006e+00,  2.0232e-01, -1.7014e+01,  6.6995e+00,  6.5935e+00],\n",
      "        [ 4.3336e-01, -7.9665e-02,  6.6995e+00, -2.6381e+00, -2.5963e+00],\n",
      "        [ 4.2650e-01, -7.8405e-02,  6.5935e+00, -2.5963e+00, -2.5552e+00]])\n",
      "25.28742790222168\n"
     ]
    }
   ],
   "source": [
    "manual_grad_A = 0.5 * torch.ger(x, x)\n",
    "\n",
    "print(manual_grad_A.data)\n",
    "print(A.grad.data)\n",
    "print(torch.norm(manual_grad_A.data - A.grad.data).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Причём тут нейросети?\n",
    "\n",
    "- Обучение с учителем (но не только!)\n",
    "- Есть данные, есть ответы\n",
    "- Нейросеть $\\approx$ сложная композиция простых функций, которая по данным восстанавливает ответы\n",
    "- Для обучения необходима целевая функция a.k.a. функция потерь или loss, который минимизируется каким-нибудь стохастическим методом первого порядка\n",
    "- Поэтому нужны градиенты!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./pytorch_logo.png\">\n",
    "\n",
    "- Удобный фреймворк для построения и обучения нейросетей\n",
    "- Реализует динамический граф вычислений\n",
    "- Вы просто пишете функцию, которая вычисляет нужное вам выражение, вычисляете (вызываете) её в некоторой точке (с некоторым аргументом) и можете получить градиенты в этой точке бесплатно!\n",
    "- Специальные инструменты для построения нейросетей: модуль с большинством популярных слоёв, для которых градиенты уже реализованы\n",
    "- Пока плохо поддерживает операции с разреженными матрицами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU vs. CPU\n",
    "\n",
    "- CPU – мощный вычислитель, но мало памяти доступно здесь и сейчас (см. лекцию про иерархию памяти)\n",
    "- GPU – ОЧЕНЬ много ядер для выполнения простых операций\n",
    "- CPU распределяет задачи по ядрам, GPU - по кластерам ядер\n",
    "\n",
    "Только использование GPU (или [TPU](https://habr.com/ru/post/422317/)) позволит за разумное время обучить сложные модели для задач компьютерного зрения, NLP, RL и других областей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Какие операции нужны для работы нейросетей?\n",
    "\n",
    "- Умножение матрицы на вектор и векторное сложение - полносвязный слой\n",
    "- Поэлементые операции - нелинейности\n",
    "- Локальные операции типа [MaxPooling](https://deepai.org/machine-learning-glossary-and-terms/max-pooling)\n",
    "- Получение из вектора или матрицы скаляра"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пример реализации операции матричного умножения\n",
    "\n",
    "- Пример исходного кода, реализующего матричное умножение на CUDA, доступен [тут](https://www.quantstart.com/articles/Matrix-Matrix-Multiplication-on-the-GPU-with-Nvidia-CUDA)\n",
    "- Сравнение с помощью PyTorch доступно по ссылке ниже [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1tHGFnvgoKnrq2C955wwdfloARnqTNsBr?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Резюме\n",
    "\n",
    "- В основе нейросетей как и других вычислительных технологий лежит линейная алгебра\n",
    "- Современные фреймворки (PyTorch, etc) позволяют вычислять градиенты автоматически\n",
    "- GPU быстрее CPU для выполнения большого числа простых операций\n",
    "- TPU ещё быстрее, но менее доступны\n",
    "- [Colab](https://colab.research.google.com/) позволяет экспериментировать с этими технологиями удалённо"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
